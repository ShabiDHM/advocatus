# FILE: backend/app/services/llm_service.py
# PHOENIX PROTOCOL - TIERED HYBRID INTELLIGENCE (Cloud -> Local -> Static)
# 1. TIER 1 (Cloud): Groq/Llama-70b (High Precision/IQ).
# 2. TIER 2 (Local): Ollama/Llama-8b (Zero Cost, High Availability).
# 3. TIER 3 (Static): Safe fallback messages (No Crashes).

import os
import json
import logging
import httpx
import re
from typing import List, Dict, Any, Optional
from groq import Groq

logger = logging.getLogger(__name__)

# --- CONFIGURATION ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL_NAME = "llama-3.3-70b-versatile" 

# Local LLM Config (Internal Docker Network)
OLLAMA_URL = os.environ.get("LOCAL_LLM_URL", "http://local-llm:11434/api/generate")
LOCAL_MODEL_NAME = "llama3"

_client: Groq | None = None

def initialize_llm_client():
    global _client
    if _client: return
    if not GROQ_API_KEY: 
        logger.warning("GROQ_API_KEY not set. System will default to Local LLM.")
        return
    try:
        _client = Groq(api_key=GROQ_API_KEY)
    except Exception as e:
        logger.critical(f"Failed to initialize Groq Client: {e}")

def get_llm_client() -> Optional[Groq]:
    if _client is None: initialize_llm_client()
    return _client

# --- HELPER: ROBUST JSON PARSER ---
def _parse_json_safely(content: str) -> Dict[str, Any]:
    """Attempts to clean and parse JSON from LLM output (which might include markdown)."""
    try:
        # Fast path
        return json.loads(content)
    except json.JSONDecodeError:
        # Strip Markdown ```json ... ```
        match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', content, re.DOTALL)
        if match:
            try:
                return json.loads(match.group(1))
            except: pass
        
        # Try finding first { and last }
        start = content.find('{')
        end = content.rfind('}')
        if start != -1 and end != -1:
            try:
                return json.loads(content[start:end+1])
            except: pass
            
        raise ValueError("Could not extract valid JSON from LLM response.")

# --- TIER 2: LOCAL ENGINE ---
def _call_local_llm(prompt: str, json_mode: bool = False) -> str:
    """Calls the internal Ollama instance."""
    logger.info(f"ğŸ”„ Switching to LOCAL LLM ({LOCAL_MODEL_NAME})...")
    try:
        payload = {
            "model": LOCAL_MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": 0.1, "num_ctx": 4096},
            "format": "json" if json_mode else None
        }
        
        with httpx.Client(timeout=60.0) as client:
            response = client.post(OLLAMA_URL, json=payload)
            response.raise_for_status()
            data = response.json()
            return data.get("response", "")
            
    except Exception as e:
        logger.warning(f"âš ï¸ Local LLM Failed: {e}")
        return ""

# --- CORE SERVICES ---

def generate_summary(text: str) -> str:
    """
    Tier 1: Groq (Best Quality)
    Tier 2: Local LLM (Fallback)
    Tier 3: Static Message
    """
    truncated_text = text[:8000]
    prompt = f"""
    You are a professional legal assistant.
    Summarize the following legal document in a concise, professional manner (max 1 paragraph).
    
    INSTRUCTIONS:
    - Keep it factual.
    - Write in the SAME LANGUAGE as the source text (Albanian/English/Serbian).
    
    Document Text:
    {truncated_text}
    """
    
    # TIER 1: CLOUD
    client = get_llm_client()
    if client:
        try:
            logger.info("â˜ï¸  Generating Summary via Groq...")
            completion = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model=GROQ_MODEL_NAME,
                temperature=0.3
            )
            return completion.choices[0].message.content or ""
        except Exception as e:
            logger.warning(f"âš ï¸ Groq Summary Failed (Limit/Error): {e}")
            # Fall through to Tier 2

    # TIER 2: LOCAL
    local_summary = _call_local_llm(prompt, json_mode=False)
    if local_summary:
        return "[Generated by Local AI] " + local_summary

    # TIER 3: STATIC
    return "PÃ«rmbledhja nuk Ã«shtÃ« e disponueshme pÃ«r momentin (Kufizim Teknik)."

def extract_findings_from_text(text: str) -> List[Dict[str, Any]]:
    """
    Tier 1: Groq (JSON Mode)
    Tier 2: Local LLM (JSON Mode attempt)
    Tier 3: Empty List
    """
    truncated_text = text[:8000]
    system_prompt = """
    Extract critical legal findings from the text below.
    Return ONLY valid JSON.
    Format: {"findings": [{"finding_text": "...", "source_text": "..."}]}
    """
    user_prompt = f"Extract findings from:\n\n{truncated_text}"

    # TIER 1: CLOUD
    client = get_llm_client()
    if client:
        try:
            logger.info("â˜ï¸  Extracting Findings via Groq...")
            completion = client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                model=GROQ_MODEL_NAME,
                response_format={"type": "json_object"},
                temperature=0.1,
            )
            content = completion.choices[0].message.content
            if content:
                data = json.loads(content)
                return data.get("findings", [])
        except Exception as e:
            logger.warning(f"âš ï¸ Groq Findings Failed: {e}")
            # Fall through to Tier 2

    # TIER 2: LOCAL
    full_prompt = f"{system_prompt}\n\nUser Input:\n{user_prompt}"
    local_content = _call_local_llm(full_prompt, json_mode=True)
    
    if local_content:
        try:
            data = _parse_json_safely(local_content)
            findings = data.get("findings", [])
            logger.info(f"âœ… Recovered {len(findings)} findings via Local LLM.")
            return findings
        except Exception as e:
            logger.error(f"âŒ Local JSON Parse Failed: {e}")

    # TIER 3: EMPTY
    return []

# Legacy stubs
def generate_socratic_response(socratic_context: List[Dict], question: str) -> Dict:
    return {"answer": "Socratic response logic.", "sources": []}

def extract_deadlines_from_text(text: str) -> List[Dict[str, Any]]:
    # Can be implemented similarly if needed
    return []