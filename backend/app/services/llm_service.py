# FILE: backend/app/services/llm_service.py
# PHOENIX PROTOCOL - INGESTION INTELLIGENCE V5.2 (DEBATE JUDGE)
# 1. PROMPT STRATEGY: 'analyze_case' now uses a "Debate Judge" cognitive forcing function.
# 2. PROMPT STRATEGY: 'extract_findings' now includes few-shot examples to improve CLAIM and CONTRADICTION detection.
# 3. GOAL: Overcome AI's default summarization behavior and force true conflict analysis.

import os
import json
import logging
import httpx
import re
from typing import List, Dict, Any, Optional
from openai import OpenAI 
from groq import Groq

logger = logging.getLogger(__name__)

# --- CONFIGURATION ---
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
OPENROUTER_BASE_URL = "https://openrouter.ai/api/v1"
OPENROUTER_MODEL = "deepseek/deepseek-chat"

GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_MODEL_NAME = "llama-3.3-70b-versatile" 

OLLAMA_URL = os.environ.get("LOCAL_LLM_URL", "http://local-llm:11434/api/generate")
LOCAL_MODEL_NAME = "llama3"

# --- CLIENT INITIALIZATION ---
_deepseek_client: Optional[OpenAI] = None
_groq_client: Optional[Groq] = None

def get_deepseek_client() -> Optional[OpenAI]:
    global _deepseek_client
    if _deepseek_client: return _deepseek_client
    if DEEPSEEK_API_KEY:
        try:
            _deepseek_client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=OPENROUTER_BASE_URL)
            return _deepseek_client
        except Exception as e:
            logger.error(f"DeepSeek Init Failed: {e}")
    return None

def get_groq_client() -> Optional[Groq]:
    global _groq_client
    if _groq_client: return _groq_client
    if GROQ_API_KEY:
        try:
            _groq_client = Groq(api_key=GROQ_API_KEY)
            return _groq_client
        except Exception as e:
            logger.error(f"Groq Init Failed: {e}")
    return None

# --- HELPER: ROBUST JSON PARSER ---
def _parse_json_safely(content: str) -> Dict[str, Any]:
    try:
        return json.loads(content)
    except json.JSONDecodeError:
        match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', content, re.DOTALL)
        if match:
            try: return json.loads(match.group(1))
            except: pass
        start, end = content.find('{'), content.rfind('}')
        if start != -1 and end != -1:
            try: return json.loads(content[start:end+1])
            except: pass
        return {}

# --- EXECUTION ENGINES ---

def _call_deepseek(system_prompt: str, user_prompt: str, json_mode: bool = False) -> Optional[str]:
    client = get_deepseek_client()
    if not client: return None
    try:
        kwargs = {
            "model": OPENROUTER_MODEL,
            "messages": [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            "temperature": 0.1, 
            "extra_headers": {"HTTP-Referer": "https://juristi.tech", "X-Title": "Juristi AI"}
        }
        if json_mode: kwargs["response_format"] = {"type": "json_object"}
        response = client.chat.completions.create(**kwargs)
        return response.choices[0].message.content
    except Exception as e:
        logger.warning(f"âš ï¸ DeepSeek Call Failed: {e}")
        return None

def _call_groq(system_prompt: str, user_prompt: str, json_mode: bool = False) -> Optional[str]:
    client = get_groq_client()
    if not client: return None
    try:
        kwargs = {
            "model": GROQ_MODEL_NAME,
            "messages": [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}],
            "temperature": 0.1,
        }
        if json_mode: kwargs["response_format"] = {"type": "json_object"}
        response = client.chat.completions.create(**kwargs)
        return response.choices[0].message.content
    except Exception as e:
        logger.warning(f"âš ï¸ Groq Call Failed: {e}")
        return None

def _call_local_llm(prompt: str, json_mode: bool = False) -> str:
    logger.info(f"ğŸ”„ Switching to LOCAL LLM ({LOCAL_MODEL_NAME})...")
    try:
        payload = {
            "model": LOCAL_MODEL_NAME,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": 0.1, "num_ctx": 4096},
            "format": "json" if json_mode else None
        }
        with httpx.Client(timeout=60.0) as client:
            response = client.post(OLLAMA_URL, json=payload)
            return response.json().get("response", "")
    except Exception:
        return ""

# --- UNIVERSAL EVIDENCE ENGINE (V5.2) ---

def generate_summary(text: str) -> str:
    truncated_text = text[:20000] 
    system_prompt = (
        "Ti je Analist GjyqÃ«sor pÃ«r RepublikÃ«n e KosovÃ«s. "
        "Detyra jote Ã«shtÃ« tÃ« krijosh njÃ« pÃ«rmbledhje tÃ« qartÃ« dhe koncize tÃ« Ã§do lloj dokumenti ligjor. "
        "Fokuso te: "
        "1. KUSH janÃ« palÃ«t kryesore? "
        "2. CILI Ã«shtÃ« konflikti thelbÃ«sor? "
        "3. KUR ka ndodhur ngjarja kryesore? "
        "4. CILI Ã«shtÃ« hapi i radhÃ«s procedural (nÃ«se pÃ«rmendet)?"
    )
    user_prompt = f"DOKUMENTI:\n{truncated_text}"
    
    res = _call_deepseek(system_prompt, user_prompt)
    if res: return res
    res = _call_groq(system_prompt, user_prompt)
    if res: return res
    return _call_local_llm(f"{system_prompt}\n\n{user_prompt}") or "PÃ«rmbledhja e padisponueshme."

def extract_findings_from_text(text: str) -> List[Dict[str, Any]]:
    """
    Extracts universal legal building blocks from text, regardless of case type.
    """
    truncated_text = text[:25000]
    
    # PHOENIX V5.2 UPGRADE: Added few-shot examples to teach the AI what we want.
    system_prompt = """
    Ti je njÃ« motor pÃ«r nxjerrjen e provave (Evidence Extraction Engine) pÃ«r sistemin ligjor tÃ« KosovÃ«s.
    
    DETYRA: ShndÃ«rro tekstin e papÃ«rpunuar nÃ« njÃ« listÃ« tÃ« strukturuar tÃ« PROVAVE.
    
    KATEGORITÃ‹ E PROVAVE:
    - EVENT, EVIDENCE, CLAIM, CONTRADICTION, QUANTITY, DEADLINE.
    
    DETYRIM: Duhet tÃ« gjenerosh TÃ‹ PAKTÃ‹N 5 gjetje nÃ«se ato ekzistojnÃ« nÃ« tekst. Mos kthe pÃ«rgjigje boshe.
    
    SHEMBUJ TÃ‹ KUALITETIT TÃ‹ LARTÃ‹:
    - PÃ«r "CLAIM": {"finding_text": "Teuta pretendon se Iliri ka fshehur pasuri nÃ« kriptovaluta.", "source_text": "Ilir Krasniqi posedon pasuri tÃ« mÃ«dha tÃ« fshehura nÃ« kriptovaluta (Bitcoin)", "category": "CLAIM"}
    - PÃ«r "CONTRADICTION": {"finding_text": "KontradiktÃ« financiare: Teuta pretendon se Iliri ka pasuri, ndÃ«rsa Iliri pretendon se biznesi i tij ka falimentuar.", "source_text": "Biznesi i Ilirit ka falimentuar nÃ« vitin 2024. Ai nuk posedon kriptovaluta.", "category": "CONTRADICTION"}
    - PÃ«r "DEADLINE": {"finding_text": "Afati pÃ«r dorÃ«zimin e pasqyrave bankare Ã«shtÃ« 30 Dhjetor 2025.", "source_text": "I padituri Ilir Krasniqi urdhÃ«rohet tÃ« sjellÃ« raportet bankare... deri mÃ« datÃ« 30 Dhjetor 2025.", "category": "DEADLINE"}

    FORMATI JSON (STRIKT):
    {
      "findings": [
        {
          "finding_text": "...",
          "source_text": "...",
          "category": "..."
        }
      ]
    }
    """
    user_prompt = f"TEKSTI I DOSJES:\n{truncated_text}"

    content = _call_deepseek(system_prompt, user_prompt, json_mode=True)
    if content: return _parse_json_safely(content).get("findings", [])
    
    content = _call_groq(system_prompt, user_prompt, json_mode=True)
    if content: return _parse_json_safely(content).get("findings", [])
    
    content = _call_local_llm(f"{system_prompt}\n\n{user_prompt}", json_mode=True)
    if content: return _parse_json_safely(content).get("findings", [])
    
    return []

def extract_graph_data(text: str) -> Dict[str, List[Dict]]:
    truncated_text = text[:15000]
    system_prompt = """
    Ti je Inxhinier i Grafit Ligjor. Detyra: Krijo hartÃ«n e marrÃ«dhÃ«nieve.
    MARRÃ‹DHÃ‹NIET: ACCUSES, OWES, CLAIMS, WITNESSED, OCCURRED_ON, CONTRADICTS.
    FORMATI JSON: {"entities": [], "relations": []}
    """
    user_prompt = f"TEKSTI:\n{truncated_text}"
    content = _call_deepseek(system_prompt, user_prompt, json_mode=True)
    if content: return _parse_json_safely(content)
    content = _call_groq(system_prompt, user_prompt, json_mode=True)
    if content: return _parse_json_safely(content)
    return {"entities": [], "relations": []}

def analyze_case_contradictions(text: str) -> Dict[str, Any]:
    """
    High-Level Strategy Analysis for the 'Analizo Rastin' Modal.
    """
    truncated_text = text[:25000]
    
    # PHOENIX V5.2 UPGRADE: The "Debate Judge" cognitive forcing strategy.
    system_prompt = """
    Ti je Gjyqtar i Debatit Ligjor. Detyra jote Ã«shtÃ« tÃ« identifikosh pikat e pÃ«rplasjes.
    
    PROCESI KOGNITIV (NDIQE ME PÃ‹RPikmÃ«ri):
    1. HAPI 1: Identifiko palÃ«n paditÃ«se/akuzuese dhe pÃ«rmblidh pretendimin e tyre kryesor nÃ« njÃ« fjali.
    2. HAPI 2: Identifiko palÃ«n e paditur/akuzuar dhe pÃ«rmblidh kundÃ«r-argumentin e tyre kryesor nÃ« njÃ« fjali.
    3. HAPI 3: Duke u bazuar VETÃ‹M nÃ« pÃ«rplasjen mes Hapave 1 dhe 2, listo kontradiktat direkte.
    4. HAPI 4: Identifiko provat kryesore qÃ« secila palÃ« pÃ«rmend pÃ«r tÃ« mbÃ«shtetur versionin e saj.
    5. HAPI 5: Identifiko informacionin kritik qÃ« mungon pÃ«r tÃ« vÃ«rtetuar njÃ«rÃ«n ose tjetrÃ«n anÃ«.
    
    OUTPUT JSON (STRIKT):
    {
        "summary_analysis": "PÃ«rmbledhje e thelbit tÃ« betejÃ«s ligjore.",
        "conflicting_parties": [
            {"party_name": "Emri i PalÃ«s 1 (PaditÃ«si/Akuzuesi)", "core_claim": "Pretendimi i tyre kryesor."},
            {"party_name": "Emri i PalÃ«s 2 (I Padituri/Akuzuari)", "core_claim": "Mbrojtja e tyre kryesore."}
        ],
        "contradictions": ["ListÃ« e qartÃ« e pikave ku versionet pÃ«rplasen."],
        "key_evidence": ["Lista e provave tÃ« pÃ«rmendura (dÃ«shmitarÃ«, dokumente)."],
        "missing_info": ["Ã‡farÃ« provash kritike duhen kÃ«rkuar pÃ«r tÃ« zgjidhur kontradiktat?"]
    }
    """
    user_prompt = f"DOSJA PÃ‹R GJYKIM:\n{truncated_text}"

    content = _call_deepseek(system_prompt, user_prompt, json_mode=True)
    if content: return _parse_json_safely(content)
    
    content = _call_groq(system_prompt, user_prompt, json_mode=True)
    if content: return _parse_json_safely(content)

    return {}

def generate_socratic_response(socratic_context: List[Dict], question: str) -> Dict:
    return {"answer": "Logic moved to R-A-G Service.", "sources": []}

def extract_deadlines_from_text(text: str) -> List[Dict[str, Any]]:
    return []